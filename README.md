# image_captioning
This repository contains an image captioning model designed to generate descriptive captions for images. The model integrates Convolutional Neural Networks (CNNs) and Transformer-based architectures to produce high-quality, coherent captions from visual input. The project encompasses the complete pipeline from data preparation and preprocessing to model training and caption generation.

The core of this project includes a CNN, specifically InceptionV3, for extracting image features, and a Transformer-based encoder-decoder framework for generating the text. This approach allows for scalable and effective captioning of images, suitable for various applications in computer vision.

## Data Preparation and Preprocessing
The initial step in preparing the data for image captioning involved creating a well-structured dataset by pairing each image with its corresponding caption. I began by loading the annotations from a local JSON file, which provided detailed metadata, including the image IDs and captions. These annotations were crucial for linking each image to its descriptive text.

The images themselves were stored in a specified directory on my Google Drive. For each entry in the annotations, I constructed the file paths to the images and verified their existence to ensure data integrity. This validation step was essential in preventing errors during the training phase, where missing files could disrupt the process.

To manage computational resources efficiently, I sampled 10,000 image-caption pairs from the full dataset. This sample size provided a balance between training efficiency and model performance, allowing for robust training within the constraints of available resources.

Once the data was organized, I performed text preprocessing on the captions. This step involved converting all text to lowercase, removing punctuation, and stripping unnecessary whitespace. Additionally, I appended special tokens [start] and [end] to each caption, marking the beginning and end of the sequence. This standardized format was critical for training the model to generate captions that start and stop appropriately.

Overall, the data preparation and preprocessing stage set a solid foundation for the subsequent model training, ensuring that the input data was clean, consistent, and ready for effective learning.

## Parameter Configuration and Tokenization Process
To prepare the data for training a machine learning model, we first configure several important parameters. The maximum sequence length is set to 40, meaning that each caption will be truncated or padded to this length. The vocabulary size is limited to 15,000 tokens, which defines the number of unique words or tokens that can be represented. A batch size of 32 is chosen to control the number of samples processed in each training step, and the model will be trained for 2 epochs, meaning that the entire dataset will be processed once during training.

The TextVectorization layer from TensorFlow is employed to convert the raw text captions into a format suitable for the model. This layer handles text preprocessing tasks such as tokenization and standardization. It takes the maximum number of tokens as VOCABULARY_SIZE and applies padding or truncation to ensure that all sequences have a consistent length of MAX_LENGTH. The tokenizer is then adapted to the dataset by learning the vocabulary from the captions, effectively mapping each unique word to an integer index. This transformation allows the model to process the text as numerical data, which is essential for training. The vocabulary_size() method provides the number of unique tokens the tokenizer has learned, which helps in understanding the scale of the vocabulary used in the model.

To save the vocabulary generated by the tokenizer, I used the pickle module to store it in a file named vocab_coco.file. This file will be useful for later reference or for reusing the vocabulary without needing to reprocess the captions. Additionally, I created two StringLookup layers from TensorFlow's Keras API: one for mapping words to indices (word2idx) and another for mapping indices back to words (idx2word). Both layers utilize the vocabulary obtained from the tokenizer. The StringLookup layers are essential for converting between text and token indices, which is necessary for feeding the data into the model during training and inference.

## Data Preparation and Pipeline Configuration
I first created a dictionary that maps image paths to their associated captions. This mapping is crucial for organizing the data in a structured format, enabling easy access and manipulation for model training and evaluation.

I then calculated and displayed the total number of unique images and captions in the dataset. To prepare the data for training and validation, I shuffled the list of image paths and split it into training and validation sets, using an 80-20 split. This ensures that the model is trained on a diverse set of images and validated on a separate subset, promoting better generalization.

The next step involved populating lists for training and validation images and captions based on the split. By extending these lists with repeated image paths corresponding to their captions, I prepared the data in a format suitable for training the model. The final output displayed the number of images and captions for both the training and validation sets, ensuring a proper distribution of data for the subsequent training phases.

 I defined a data loading and preprocessing mechanism for images and captions. The function takes an image path and a caption as inputs.

To handle the image, I first read the image file from the specified path and decode it from JPEG format. After decoding, I resized the image to a fixed size of 299x299 pixels to ensure uniformity in input dimensions. The image is then preprocessed using InceptionV3's preprocessing function, which normalizes pixel values according to the model's requirements.

For the caption, I utilized the previously defined tokenizer to convert the text into a sequence of integers. This process transforms the caption into a format suitable for input into the neural network. The function returns both the preprocessed image and the tokenized caption, ready for use in model training or evaluation.
In this section, I constructed and configured the TensorFlow datasets for both training and validation. The train_dataset and val_dataset are created from tensor slices of image paths and corresponding captions.

For each dataset, I used the tf.data.Dataset.from_tensor_slices function to create datasets from the image paths and captions. I then applied the load_data function to preprocess the images and tokenize the captions. To optimize performance, the datasets were shuffled with a buffer size and batched with a specified batch size. The use of tf.data.AUTOTUNE allows TensorFlow to automatically adjust the number of parallel calls to improve performance based on available system resources.

This setup ensures that the data pipeline is efficient, with preprocessing and batching happening in parallel to minimize bottlenecks during model training.

## Construction of Custom Transformer(Encoder, Decoder) and Embeddings Layers 
I implemented a Convolutional Neural Network (CNN) encoder using the InceptionV3 architecture. This model utilizes the pre-trained weights from ImageNet and excludes the fully connected top layers by setting include_top=False. The output of the InceptionV3 model is reshaped to flatten the feature maps into a sequence, which is necessary for further processing in the image captioning pipeline. The encoder model is then defined and returned, ready to extract features from input images.
I developed a custom Transformer Encoder layer using TensorFlow, which serves as a fundamental building block for processing sequences in Transformer models. This layer incorporates several key components:

The layer begins with Layer Normalization, applied both before and after the attention mechanism. This normalization stabilizes and accelerates the training process by normalizing the inputs and outputs of the network layers.

The Multi-Head Attention mechanism is central to the encoder layer. It enables the model to simultaneously attend to different parts of the input sequence. This multi-faceted attention allows the model to capture a wide range of dependencies and relationships within the data, enhancing its ability to understand complex patterns.

Following the attention mechanism, a Dense Layer with ReLU activation is applied. This feedforward network processes the output of the attention mechanism, allowing for further transformation and enrichment of the feature representations.

In the call method of the layer, these components are executed sequentially. First, layer normalization is applied, followed by processing through the dense layer. The attention mechanism is then applied, and another layer normalization is performed on the combined output. This structure facilitates effective sequence processing and feature extraction, contributing to the model's overall performance.

I designed a custom Embeddings layer to handle token and positional embeddings in sequence processing tasks. This layer is crucial for incorporating both the semantic meaning of tokens and their positions within the input sequences.

The layer features two primary components:

Token Embeddings,This component uses an embedding matrix to convert input token IDs into dense vectors of fixed size. These embeddings capture the semantic meaning of each token based on its context within the vocabulary.
The TransformerDecoderLayer class implements a transformer decoder layer designed for sequence-to-sequence tasks. This layer integrates several critical components to enhance the performance of a transformer-based model.

The decoder layer incorporates an embedding mechanism for token and position embeddings, essential for encoding the input sequences. It features two multi-head attention mechanisms: one for self-attention and another for cross-attention with the encoder's output. These attention layers help the model focus on different parts of the input sequence and encoder output respectively.

Layer normalization is applied after each attention mechanism to stabilize training and improve convergence. The decoder also includes two dense feed-forward layers with ReLU activation and dropout for regularization, which help the model to generalize better and prevent overfitting.

Additionally, a causal attention mask is used to ensure that each position in the sequence can only attend to earlier positions, which is crucial for autoregressive tasks. This mask is dynamically generated based on the sequence length and batch size.

In summary, this class structures the decoder to handle complex sequence-to-sequence tasks, with robust attention mechanisms and normalization techniques, ensuring effective learning and prediction capabilities.

## Image Captioning Model
The ImageCaptioningModel class integrates a Convolutional Neural Network (CNN) encoder, a transformer-based sequence encoder, and a transformer decoder into a unified architecture for generating image captions. This model leverages the CNN to extract rich features from images, which are then processed by the transformer encoder to capture complex relationships in the data. The transformer decoder generates the actual captions based on the encoder's output.

The model also supports optional image augmentation during training to improve its robustness and generalization capabilities. This feature helps in enhancing the model's performance by providing varied input data, which can make the model more resilient to different image conditions.

In terms of functionality, the ImageCaptioningModel includes several crucial methods. The calculate_loss method computes the loss between predicted and true captions while applying a mask to handle padding tokens appropriately. The calculate_accuracy method evaluates the accuracy of predictions by comparing them with the true captions, considering the mask to ignore padding.

The compute_loss_and_acc method combines the encoder and decoder to compute the loss and accuracy for a given batch of images and captions, providing an end-to-end evaluation of the model's performance. The train_step method executes the training step, handling the forward pass, loss calculation, gradient computation, and parameter updates, while also tracking the training loss and accuracy. The test_step method is used for evaluating the model's performance on validation or test datasets, updating the loss and accuracy metrics accordingly.

Overall, the ImageCaptioningModel class combines advanced techniques to create a robust solution for generating descriptive captions from images. By integrating CNNs and transformers, the model is well-equipped to handle complex multimodal tasks, providing a comprehensive approach to image captioning.

## Model Training and Saving
To facilitate model training and saving, I have configured a custom callback, SaveModelEveryEpoch, which ensures that the model's weights are saved after each epoch. This setup is crucial for managing model checkpoints efficiently, especially when training over multiple epochs.

The model is compiled with the Adam optimizer and SparseCategoricalCrossentropy loss function. The loss function is configured to handle the predictions and true labels effectively, and it is set to "none" for reduction, which allows for custom loss calculations later in the training process.

The training process is executed over 2 epochs using the fit method. The training dataset is provided along with validation data to monitor performance throughout the training process. The SaveModelEveryEpoch callback is included in the training to automatically save the model's state after each epoch. This approach helps in resuming training from the last saved state or evaluating the model's performance at different training stages.

After the training is completed, the model's weights are saved to a file named model.h5. This file will contain the learned parameters of the model, allowing for future evaluation or further training without the need to retrain from scratch. This process ensures that the model's progress is preserved and can be utilized effectively for subsequent tasks.

## Image Caption Generation
The provided code defines functions to generate captions for images using a trained model. The load_image_from_path function reads an image file, decodes it from JPEG format, resizes it to 299x299 pixels, and preprocesses it for input into the InceptionV3 model. This preprocessing is essential for ensuring that the image data is in the correct format for feature extraction by the CNN model.

The generate_caption function takes an image path and optionally adds noise to the image to test robustness. It loads and preprocesses the image, then generates embeddings using the CNN model. These embeddings are passed through the encoder to produce encoded image features.

The caption generation starts with the token '[start]' and iteratively predicts the next token in the sequence until the '[end]' token is generated or the maximum length is reached. Each token is predicted based on the current sequence and the image features, and the word corresponding to the predicted token is added to the caption. The generated caption is then cleaned by removing the '[start]' token before returning.

This setup allows for flexible and robust caption generation for new images, leveraging the pre-trained model to produce meaningful descriptions based on the image content.

##
To utilize the image captioning model, make sure to have the required dependencies installed and prepare your dataset according to the guidelines provided in this repository. The generate_caption function is designed to produce captions for new images using the trained model, providing a straightforward way to test the model's performance.

For example, you can generate a caption for an image by specifying the image path and calling the function. Ensure that the path to the image is correct and the image is properly formatted.
