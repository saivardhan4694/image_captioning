{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saivardhan4694/image_captioning/blob/main/image_captioning_on_coco_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWmqJLPGC8w7",
        "outputId": "82a67c87-ef01-4dad-c769-c9f98a086604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import random\n",
        "import requests\n",
        "import json\n",
        "from math import sqrt\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "hcqJHIMoECNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: pip install pycocotools\n",
        "\n",
        "!pip install pycocotools\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCadoVMsEby7",
        "outputId": "daec8dbc-0aaf-4da7-c804-f45e5697992b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Define local paths\n",
        "ANNOTATIONS_PATH = '/content/drive/MyDrive/Colab Notebooks/captions_train2017.json'\n",
        "IMAGES_PATH = '/content/drive/MyDrive/Colab Notebooks/images_dir'\n",
        "\n",
        "# Load annotations from the local JSON file\n",
        "with open(ANNOTATIONS_PATH, 'r') as f:\n",
        "    data = json.load(f)\n",
        "    data = data['annotations']\n",
        "\n",
        "# Create image-caption pairs\n",
        "img_cap_pairs = []\n",
        "for sample in data:\n",
        "    img_name = '%012d.jpg' % sample['image_id']\n",
        "    img_path = os.path.join(IMAGES_PATH, img_name)\n",
        "    if os.path.exists(img_path):  # Check if the image exists\n",
        "        img_cap_pairs.append([img_path, sample['caption']])\n",
        "\n",
        "# Create a DataFrame with the image paths and captions\n",
        "captions = pd.DataFrame(img_cap_pairs, columns=['image', 'caption'])\n",
        "\n",
        "# Sample up to 10,000 image-caption pairs if available\n",
        "num_samples = min(10000, len(captions))\n",
        "captions = captions.sample(num_samples).reset_index(drop=True)\n",
        "\n",
        "# Validate if all image paths exist\n",
        "def validate_image_paths(df):\n",
        "    valid_paths = df['image'].apply(os.path.exists)\n",
        "    return df[valid_paths]\n",
        "\n",
        "# Filter out non-existent image paths\n",
        "captions = validate_image_paths(captions)\n",
        "\n",
        "# Display the first few rows\n",
        "print(captions.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdcK-SLiEINr",
        "outputId": "99d84b2d-d23c-45a5-8f75-9dc9f63c36d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               image  \\\n",
            "0  /content/drive/MyDrive/Colab Notebooks/images_...   \n",
            "1  /content/drive/MyDrive/Colab Notebooks/images_...   \n",
            "2  /content/drive/MyDrive/Colab Notebooks/images_...   \n",
            "3  /content/drive/MyDrive/Colab Notebooks/images_...   \n",
            "4  /content/drive/MyDrive/Colab Notebooks/images_...   \n",
            "\n",
            "                                             caption  \n",
            "0  Several people sitting around a living room pl...  \n",
            "1    a person walking on a street talking on a phone  \n",
            "2       A man knelt down next to two small children.  \n",
            "3           Two men next to each other making faces.  \n",
            "4  red and white streaks in the road coming off o...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    text = '[start] ' + text + ' [end]'\n",
        "    return text\n",
        "\n",
        "captions['caption'] = captions['caption'].apply(preprocess)\n",
        "captions.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SBtEnpKhMmNA",
        "outputId": "76d9e95b-4d47-4071-cced-e0b23e23c025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               image  \\\n",
              "0  /content/drive/MyDrive/Colab Notebooks/images_...   \n",
              "1  /content/drive/MyDrive/Colab Notebooks/images_...   \n",
              "2  /content/drive/MyDrive/Colab Notebooks/images_...   \n",
              "3  /content/drive/MyDrive/Colab Notebooks/images_...   \n",
              "4  /content/drive/MyDrive/Colab Notebooks/images_...   \n",
              "\n",
              "                                             caption  \n",
              "0  [start] several people sitting around a living...  \n",
              "1  [start] a person walking on a street talking o...  \n",
              "2  [start] a man knelt down next to two small chi...  \n",
              "3  [start] two men next to each other making face...  \n",
              "4  [start] red and white streaks in the road comi...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-171d961c-152d-4a90-91ff-ea6a55c15ec1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/images_...</td>\n",
              "      <td>[start] several people sitting around a living...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/images_...</td>\n",
              "      <td>[start] a person walking on a street talking o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/images_...</td>\n",
              "      <td>[start] a man knelt down next to two small chi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/images_...</td>\n",
              "      <td>[start] two men next to each other making face...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/Colab Notebooks/images_...</td>\n",
              "      <td>[start] red and white streaks in the road comi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-171d961c-152d-4a90-91ff-ea6a55c15ec1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-171d961c-152d-4a90-91ff-ea6a55c15ec1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-171d961c-152d-4a90-91ff-ea6a55c15ec1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b38ab2ed-8b5d-4732-b564-ed9be500676e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b38ab2ed-8b5d-4732-b564-ed9be500676e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b38ab2ed-8b5d-4732-b564-ed9be500676e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "captions",
              "summary": "{\n  \"name\": \"captions\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6640,\n        \"samples\": [\n          \"/content/drive/MyDrive/Colab Notebooks/images_dir/000000048708.jpg\",\n          \"/content/drive/MyDrive/Colab Notebooks/images_dir/000000487567.jpg\",\n          \"/content/drive/MyDrive/Colab Notebooks/images_dir/000000483992.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9947,\n        \"samples\": [\n          \"[start] a peeled old banana peeling lying in the street [end]\",\n          \"[start] a man riding a skateboard while holding a brown dog [end]\",\n          \"[start] a plate of food with beans broccoli and greens [end]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 40\n",
        "VOCABULARY_SIZE = 15000\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 1000\n",
        "EMBEDDING_DIM = 512\n",
        "UNITS = 512\n",
        "EPOCHS = 1\n",
        "\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCABULARY_SIZE,\n",
        "    standardize=None,\n",
        "    output_sequence_length=MAX_LENGTH)\n",
        "\n",
        "tokenizer.adapt(captions['caption'])\n",
        "tokenizer.vocabulary_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24o8ONVtM1EL",
        "outputId": "149ce3f4-10ab-4a28-cfc2-d30bc60a8d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5017"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(tokenizer.get_vocabulary(), open('vocab_coco.file', 'wb'))\n",
        "\n",
        "word2idx = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary())\n",
        "\n",
        "idx2word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True)\n"
      ],
      "metadata": {
        "id": "rgGIlq9IM4tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import random\n",
        "\n",
        "\n",
        "img_to_cap_vector = collections.defaultdict(list)\n",
        "for img, cap in zip(captions['image'], captions['caption']):\n",
        "    img_to_cap_vector[img].append(cap)\n",
        "\n",
        "\n",
        "print(f\"Total unique images: {len(img_to_cap_vector)}\")\n",
        "print(f\"Total captions: {sum(len(caps) for caps in img_to_cap_vector.values())}\")\n",
        "\n",
        "\n",
        "img_keys = list(img_to_cap_vector.keys())\n",
        "random.shuffle(img_keys)\n",
        "\n",
        "\n",
        "slice_index = int(len(img_keys) * 0.8)\n",
        "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
        "\n",
        "\n",
        "print(f\"Number of training images: {len(img_name_train_keys)}\")\n",
        "print(f\"Number of validation images: {len(img_name_val_keys)}\")\n",
        "\n",
        "\n",
        "train_imgs = []\n",
        "train_captions = []\n",
        "for imgt in img_name_train_keys:\n",
        "    capt_len = len(img_to_cap_vector[imgt])\n",
        "    train_imgs.extend([imgt] * capt_len)\n",
        "    train_captions.extend(img_to_cap_vector[imgt])\n",
        "\n",
        "\n",
        "print(f\"Number of training images: {len(train_imgs)}\")\n",
        "print(f\"Number of training captions: {len(train_captions)}\")\n",
        "\n",
        "\n",
        "val_imgs = []\n",
        "val_captions = []\n",
        "for imgv in img_name_val_keys:\n",
        "    capv_len = len(img_to_cap_vector[imgv])\n",
        "    val_imgs.extend([imgv] * capv_len)\n",
        "    val_captions.extend(img_to_cap_vector[imgv])\n",
        "\n",
        "\n",
        "print(f\"Number of validation images: {len(val_imgs)}\")\n",
        "print(f\"Number of validation captions: {len(val_captions)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRjqYEKkM7-N",
        "outputId": "f4ea614a-615a-4672-b0a7-28a437c4c572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique images: 6640\n",
            "Total captions: 10000\n",
            "Number of training images: 5312\n",
            "Number of validation images: 1328\n",
            "Number of training images: 8013\n",
            "Number of training captions: 8013\n",
            "Number of validation images: 1987\n",
            "Number of validation captions: 1987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(img_path, caption):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    caption = tokenizer(caption)\n",
        "    return img, caption"
      ],
      "metadata": {
        "id": "1zIQhibXOZCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_imgs, train_captions))\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (val_imgs, val_captions))\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "piU54FSYQRlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "        tf.keras.layers.RandomRotation(0.2),\n",
        "        tf.keras.layers.RandomContrast(0.3),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "GHKlJ-6DQSR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_Encoder():\n",
        "    inception_v3 = tf.keras.applications.InceptionV3(\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    output = inception_v3.output\n",
        "    output = tf.keras.layers.Reshape(\n",
        "        (-1, output.shape[-1]))(output)\n",
        "\n",
        "    cnn_model = tf.keras.models.Model(inception_v3.input, output)\n",
        "    return cnn_model"
      ],
      "metadata": {
        "id": "MPZevB-gQX-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
        "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n",
        "\n",
        "\n",
        "    def call(self, x, training):\n",
        "        x = self.layer_norm_1(x)\n",
        "        x = self.dense(x)\n",
        "\n",
        "        attn_output = self.attention(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x,\n",
        "            attention_mask=None,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        x = self.layer_norm_2(x + attn_output)\n",
        "        return x"
      ],
      "metadata": {
        "id": "nNRvmzzgQadx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, max_len):\n",
        "        super().__init__()\n",
        "        self.token_embeddings = tf.keras.layers.Embedding(\n",
        "            vocab_size, embed_dim)\n",
        "        self.position_embeddings = tf.keras.layers.Embedding(\n",
        "            max_len, embed_dim, input_shape=(None, max_len))\n",
        "\n",
        "\n",
        "    def call(self, input_ids):\n",
        "        length = tf.shape(input_ids)[-1]\n",
        "        position_ids = tf.range(start=0, limit=length, delta=1)\n",
        "        position_ids = tf.expand_dims(position_ids, axis=0)\n",
        "\n",
        "        token_embeddings = self.token_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        return token_embeddings + position_embeddings"
      ],
      "metadata": {
        "id": "CHpdb0C3QcTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, embed_dim, units, num_heads):\n",
        "        super().__init__()\n",
        "        self.embedding = Embeddings(\n",
        "            tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)\n",
        "\n",
        "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "\n",
        "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
        "        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)\n",
        "\n",
        "        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=\"softmax\")\n",
        "\n",
        "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
        "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "\n",
        "    def call(self, input_ids, encoder_output, training, mask=None):\n",
        "        embeddings = self.embedding(input_ids)\n",
        "\n",
        "        combined_mask = None\n",
        "        padding_mask = None\n",
        "\n",
        "        if mask is not None:\n",
        "            causal_mask = self.get_causal_attention_mask(embeddings)\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "\n",
        "        attn_output_1 = self.attention_1(\n",
        "            query=embeddings,\n",
        "            value=embeddings,\n",
        "            key=embeddings,\n",
        "            attention_mask=combined_mask,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        out_1 = self.layernorm_1(embeddings + attn_output_1)\n",
        "\n",
        "        attn_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_output,\n",
        "            key=encoder_output,\n",
        "            attention_mask=padding_mask,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        out_2 = self.layernorm_2(out_1 + attn_output_2)\n",
        "\n",
        "        ffn_out = self.ffn_layer_1(out_2)\n",
        "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
        "        ffn_out = self.ffn_layer_2(ffn_out)\n",
        "\n",
        "        ffn_out = self.layernorm_3(ffn_out + out_2)\n",
        "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
        "        preds = self.out(ffn_out)\n",
        "        return preds\n",
        "\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n"
      ],
      "metadata": {
        "id": "6uKSE1JtQjDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.image_aug = image_aug\n",
        "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n",
        "\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "    def compute_loss_and_acc(self, img_embed, captions, training=True):\n",
        "        encoder_output = self.encoder(img_embed, training=True)\n",
        "        y_input = captions[:, :-1]\n",
        "        y_true = captions[:, 1:]\n",
        "        mask = (y_true != 0)\n",
        "        y_pred = self.decoder(\n",
        "            y_input, encoder_output, training=True, mask=mask\n",
        "        )\n",
        "        loss = self.calculate_loss(y_true, y_pred, mask)\n",
        "        acc = self.calculate_accuracy(y_true, y_pred, mask)\n",
        "        return loss, acc\n",
        "\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        imgs, captions = batch\n",
        "\n",
        "        if self.image_aug:\n",
        "            imgs = self.image_aug(imgs)\n",
        "\n",
        "        img_embed = self.cnn_model(imgs)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss, acc = self.compute_loss_and_acc(\n",
        "                img_embed, captions\n",
        "            )\n",
        "\n",
        "        train_vars = (\n",
        "            self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "        )\n",
        "        grads = tape.gradient(loss, train_vars)\n",
        "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        imgs, captions = batch\n",
        "\n",
        "        img_embed = self.cnn_model(imgs)\n",
        "\n",
        "        loss, acc = self.compute_loss_and_acc(\n",
        "            img_embed, captions, training=False\n",
        "        )\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]"
      ],
      "metadata": {
        "id": "32uspYrXQnqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)\n",
        "decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n",
        "\n",
        "cnn_model = CNN_Encoder()\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckjdVaV-QpaK",
        "outputId": "e101bae4-c03d-4667-de16-c84886697f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class SaveModelEveryEpoch(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, save_dir):\n",
        "        super(SaveModelEveryEpoch, self).__init__()\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Save the model after every epoch\n",
        "        epoch_save_path = f'{self.save_dir}/model_epoch_{epoch + 1}.keras'\n",
        "        self.model.save(epoch_save_path)\n",
        "        print(f'\\nModel saved to: {epoch_save_path}')\n"
      ],
      "metadata": {
        "id": "WQSwo2nLn8M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = '/content/drive/MyDrive/Colab Notebooks/saved_models'\n",
        "\n",
        "\n",
        "save_model_callback = SaveModelEveryEpoch(save_dir)\n",
        "\n",
        "\n",
        "caption_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=\"none\")\n",
        ")\n",
        "\n",
        "\n",
        "history = caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=2,  # Run for 2 epochs\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[save_model_callback]\n",
        ")\n",
        "\n",
        "caption_model.save_weights('model.h5')"
      ],
      "metadata": {
        "id": "VOSKT_h9QteU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_from_path(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def generate_caption(img_path, add_noise=False):\n",
        "    img = load_image_from_path(img_path)\n",
        "\n",
        "    if add_noise:\n",
        "        noise = tf.random.normal(img.shape)*0.1\n",
        "        img = img + noise\n",
        "        img = (img - tf.reduce_min(img))/(tf.reduce_max(img) - tf.reduce_min(img))\n",
        "\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    img_embed = caption_model.cnn_model(img)\n",
        "    img_encoded = caption_model.encoder(img_embed, training=False)\n",
        "\n",
        "    y_inp = '[start]'\n",
        "    for i in range(MAX_LENGTH-1):\n",
        "        tokenized = tokenizer([y_inp])[:, :-1]\n",
        "        mask = tf.cast(tokenized != 0, tf.int32)\n",
        "        pred = caption_model.decoder(\n",
        "            tokenized, img_encoded, training=False, mask=mask)\n",
        "\n",
        "        pred_idx = np.argmax(pred[0, i, :])\n",
        "        pred_idx = tf.convert_to_tensor(pred_idx)\n",
        "        pred_word = idx2word(pred_idx).numpy().decode('utf-8')\n",
        "        if pred_word == '[end]':\n",
        "            break\n",
        "\n",
        "        y_inp += ' ' + pred_word\n",
        "\n",
        "    y_inp = y_inp.replace('[start] ', '')\n",
        "    return y_inp"
      ],
      "metadata": {
        "id": "A7OMMs5hAHWt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}